# PanoramicTracking
Real-time panoramic video stitching and object tracking

## TODO

- [x] 完善`base`库，加入线程安全的队列等常用的数据结构
- [x] 完善`log`库，对于每次运行的结果，不要用`imshow`，而是根据运行时间创个文件夹，把所有中间结果以图片的形式保存下来
- [x] 学习常用的图像融合算法
  - [x] 加权融合
  - [x] 羽化融合
  - [x] 最佳拼接缝融合
  - [ ] Multiband融合
- [x] 跑通`openCV`里的sitch的例子，看看Bundle Adjustment和Wave Correct到底作用大不大
- [x] 比较自己实现的拼接和`openCV`里的sitch的例子的区别
- [ ] 弄明白Wave Correct、Bundle Adjustment的原理
- [ ] 仔细看一下`cv::detail`里的各个拼接模块怎么用，能够做到自由地组合并添加新的算法进去
- [x] 试一下kaggle、colab、aistudio之类的免费云服务器
- [ ] **找出图像拼接、单应估计领域近几年的所有顶刊顶会论文阅读**
- [x] 开发图像采集模块
- [x] 学习DLT、Tensor DLT、STN(空间变换层)等基础知识（可以看下硕士毕业论文）
- [ ] 了解下SFM、光流法（看到很多单应估计领域的模型用到了这些东西）
- [x] Pytorch复习



## 常见的图像融合算法

1.加权平均融合：I-fused(x,y) = α⋅I1(x,y) + (1−α)⋅I2(x,y)

- 该融合算法2张图片的权重是固定的，相加为1



2.羽化融合（也叫渐入检出的融合）：I-fused(x,y) = w(x,y)⋅I1(x,y) + (1−w(x,y))⋅I2(x,y)

- 加权平均融合的改进版，重叠区不同位置，2张图所占的权重不一样



3.最佳拼接缝（Optimal Seam）融合（也叫最佳缝合线）

- 在两幅图像的重叠区域内，通过特定的算法找到一条“最佳”分割线，分割线是2张图片差异最小的一条路径（重叠区的内容不同时Seam可能需要动态调整）。分割线的两侧分别保留对应图像的像素，从而尽量避免颜色、亮度或内容的不一致

- 分割线的寻找有多种方式，比如：逐点法、动态规划法、图割法、深度学习......
- 最佳拼接缝可以与别的图像融合算法结合，比如在缝左右小范围内的像素使用羽化融合

> Optimal Seam在一些论文里被看为Image Alignment的方法，可以单独用，也可以和全局、局部透视变换结合
>



## 改进思路

1.用深度学习替代传统的类似SIFT那些的特征点检测算法，唯一要确认的是深度学习找的特征点和用传统算法找的，在数据结构上有没有区别

- 能用的论文：SuperPoint、LF-Net、D2-Net（这都只是找特征点的论文，不晓得有没有人把他们加入拼接的框架能参考一下）

2.针对监控视频晚上照度低、特征点难以提取的缺点，引入图像增强算法来对低照度图片进行拼接

3.用基于深度学习的image warping（比如单应性估计）替代基于特征点的方法

4.结合语义分割技术，用以确定特征点搜索的ROI，从而减少前景的影响

5.针对低重叠率的输入图片，设计个找重叠区域的mask模块，以mask的形式滤除重叠区之外的部分，然后将仅将重叠部分输入单应估计模块，这个模块主要就解决“误对齐”问题，不是没对好有鬼影，而是完全对齐错东西了

6.将现有的模块改成级联的形式

7.参考Content-Aware...那篇论文，设计一个模块能生成一个Mask来获得主导拼接的平面，同时滤掉低纹理、运动、低照度的物体

8.参考20年和21年2篇图像拼接的论文，搞个内容矫正模块，不过这个模块貌似只能用用合成的数据集来监督学习，没法无监督学习

9.微调视觉大模型backbone比如Vit、DINOv2代替CNN进行单应估计



## 开发遇到的问题

1.plog库在子模块中（比如Sensor）输出不了日志

- 原因：plog实际上是全局共享了一个静态变量，但是动态库和依赖它的可执行文件不共享全局变量，所以即使在主进程的`main()`中初始化了一个plog实例(通过`plog::init()`)，动态库里面还是访问不到这个实例
- 解决办法：动态库中独立初始化一个plog实例



2.海康相机的SDK怎么添加？

- 它那个文档写的有问题，除了必须添加的`HCNet`和`HCCore`以外，还有这2个库所依赖的那些动态库都得放到`bin`路径下，只不过那些库不用写在`CMakeLists.txt`里面
- 文档里说HKCom文件夹不能改名，2类库得分开放也是错的。可以把所有`.lib`和`.dll`放在一个目录中，CMake添加那些`.lib`就行了



3.CMake多个库循环添加，比如我的项目中有动态库A和可执行文件B都依赖了OpenCV，同时B又依赖A，这会造成多次添加OpenCV这个库的问题

- 解决办法：在`target_link_libraries`时，用`PRIVATE`来添加依赖，这样B在添加库A的时候，就不会再添加A依赖的库了



4.如何把定义在父级CMakeLists里的变量，在子CMakeLists更改并同步

- 如果通过`add_subdirectory`添加了子CMakeLists，在其中虽然可以访问父CMakeLists中的变量，但是修改之后不会同步到父CMakeLists。有时候我们在子CMakeLists新建了个动态库，希望把他同步到全局的动态库列表中，所以就出现了该问题
- 解决办法：

```cmake
list(APPEND ALL_LIBS lib_common)		# 先修改变量
set (ALL_LIBS ${ALL_LIBS} PARENT_SCOPE) # 把变量同步到父CMakeLists中
```





## 一些疑问

1.多图拼接时，哪张图片为基准图像？

- 之前的想法，包括参考代码2的实现都是假设从左到右有N个相机，以最左或最右的相机为得到的图像为基准。但是实际上OpenCV的图像拼接模块并不是这样实现的，它会选择一张与其他图像重叠区域最大、匹配特征点最多的图像作为基准图像，可以通过打印`estimate`得到的各相机的外参来知道谁到底是基准（R为单位阵）



2.为什么要进行柱面/球面之类的图像投影?

- 多个相机拍摄时，相机离物体的距离不同（各相机不是在一个平面上的），导致图像中物体的尺度不同。通过图像投影，可以把各相机的成像画面统一到一个坐标系下，通过在图像投影之后再进行图像的对齐和融合。



3.OpenCV的图像拼接模块`cv::detail`里面为什么用相机的外参来描述2张图像之间的几何变换关系而不用单应矩阵，并且为什么它可以在不要标定板的情况下求出相机的内外参？

- 首先，如果要准确的求出相机的内外参，肯定是需要标定板的，`cv::detail::Esitmator`主要是为了求相机的外参，内参是随便设的一个固定的值（光心为图像的中心，焦距为）
- 单应矩阵可以拆成2个相机内外参的乘积：H10=K1·R1·R0^-1·K0^-1，所以在**内参固定**的情况下，用单应矩阵和用外参来表示2个图像之间的几何变换关系是等价的

[Opencv2.4.9源码分析——Stitching（三）_opencv stitching-CSDN博客](https://blog.csdn.net/zhaocj/article/details/78809143)



4.mosaic拼接和panoramic拼接的区别是什么？

- 主要区别在于场景和相机的运动：
  - mosaic拼接主要针对的是拼接场景近似**平面**（例如墙面、地面、文档、远处物体），且相机的移动主要是**平移**（也可小角度的旋转）
  - panoramic拼接主要是针对**三维**的拼接场景（例如室内场景、近处物体），且相机的运动主要是是**旋转运动**（绕固定点的水平或垂直旋转），或者视角覆盖范围较大

> 只有panoramic拼接才需要柱面、球面等投影，mosaic拼接则不需要
>



5.图像拼接的步骤是什么？

(1)图像获取 → (2) 图像校准（可选） → (3) 图像配准 → (4) 图像对齐 → (5) 图像融合 → (6) 全局优化（可选） 

- 全局优化包括Bundle Adjustment、Wave Correct、矩形化...很多论文里貌似都没有全局优化，但是这2个步骤非常重要，**很影响观感**



6.到时候论文里实验部分怎么设计？用什么指标？该对比多少算法？怎么选择要对比的算法？

> 看《研究生自救指南》

- 实验还是和别的CV领域一样，一般得包括：**对比实验**、**消融实验**、**实例分析**。参考UDIS这篇论文，这些实验可以分为以下2类：
  - 定量实验：数值指标的分析
  - 定性实验：主要是看可视化拼接结果的细节，比如相同图片拼接，不同算法对于鬼影之类的处理效果
- **定量**实验**指标的选择**主要还是看改进是什么方面
  - 图像配准算法：特征点匹配对数、匹配准确率
  - 图像配准+对齐：峰值信噪比（PSNR）、结构相似性（SSIM）
  - 图像融合算法：峰值信噪比（PSNR）、结构相似性（SSIM）
  - 其他：运行速度
- 消融实验怎么设计：比如我的模型是在UDIS的基础上改进的，加了A、B、C 3个模块，那么消融实验就需要UDIS、UDIS+A、UDIS+A+B、UDIS+A+B+C来验证改进的有效性

- 对比实验到时候选择**4个**（参考UDIS这篇论文的数量）左右的开源算法就行了，具体选什么算法后面再看吧...



7.实验到底是定性分析还是定量分析还是都进行呢？

- 最优方案肯定是都进行，参考UDIS这篇论文，对比实验至少要都进行，消融实验可以只定性分析



8.论文中提到的“基线”是什么意思？比如“大基线深度单应性网络”

- “基线”指的是2个相机之间的**相对位置**和**角度**之间的差异。大基线就说明2个相机的位置和角度差异比较大



9.什么是视差？

- 视差指的是由两个不同视角拍摄的图像中**同一物体的对应点**在水平或垂直方向上的**像素差异**。产生原因：2个相机有一定的基线
- 视差与物体的深度有直接的关系：2者大小成反比



10.怎么设计损失函数？

- 这个问题应该是模型设计的最关键的问题了，当我们验证一个idea有没有用的时候，模型本身其实初期可以没必要搞那么复杂，随便搞个VGG就可以了，这个时候可能最主要的是验证我们设计的损失到底work不。

- 回到损失函数如何设计，如果我们现在是把一个传统的问题用深度学习来做，那么首先可以看看传统问题本身是不是用**优化**的思路来做的，如果是的话，就可以参考传统优化问题中的**能量函数**是针对什么进行约束的，直接对照这些能量函数的目的，来设计损失函数。比如图像矩形化，传统方法的能量函数包括对直线的约束、边界的约束、形状的约束，那我用深度学习的损失函数可以也包含这几个约束项，但是计算公式可能就需要重新设计

- 设计损失函数的难度不是公式用什么，而是对什么进行约束，一定要先想好目的（对什么东西进行约束），然后再设计。

  比如进行单应估计时，我们可以对以下内容进行约束：

  - 用估计的H进行warp后的角点和GT Hwarp后的角点尽可能保持一致：那么就可以用角点偏移的L2范数作为损失函数。
  - warp后target image和reference image在重叠区的内容更加一致：那么就可以用像素的差值L1范数来做损失函数...

> 对同一个目的（约束）我们有不同的实现形式，比如内容的一致性约束，我们可以用像素L1范数来作为损失函数，也可以用某层feature map的L1范数来作为损失函数。所以，如果要以损失函数作为闯进点的话，有以下2个思路：
>
> - 设计个新的约束项
> - 对已有约束项提出一种新的、更有效的表达式

- 设计的损失函数如果有多个约束项，要注意各项的权重，一直加入正则化



11.什么是image warp，他和单应变换有啥关系 ？

- image warp就是对一副图像进行一些变换，单应矩阵是图像拼接领域最场景的一个warp方法，除此之外，还有许多warp model，比如基于mesh、optical flow、thin plate spline...



12.合成数据集和真实数据集的最大的区别是什么？

- 合成数据集即使对其中一张图片做了单应变换，拍摄2张图片的相机光心还是一致的，所以是没有视差（Parallax）的。但是实际真实数据中很多图片对是存在视差的



13.什么是cost volume？

cost volume的中文是**代价张量**，它是一个多维数据结构，用来描述输入的2幅图像的像素之间的匹配代价。常用于立体匹配、光流估计、深度估计等任务中。

- cost volume的维度通常是(H, W, D). 其中H、W表示输入图片的宽、高，D表示计算匹配代价的像素点的范围。我们计算通常不仅仅计算图1某像素点(x,y)与在图2相同位置(x,y)的像素的相似度，还会计算与图2的(x,y)点周围像素的相似度，比如VFISNet中，会计算图2中对应位置像素点以r为半径的圆内的所有像素。
- 匹配代价其实就是相似度，一般用向量的点积、L1范数、互相关等操作计算，也可用神经网络直接预测



14.cost volume或者21年那篇论文提出的Contexual Correlation Layer怎么用？

- 用于将reference image和target image的特征进行融合。之前的特征融合可能就是个concatenate操作，直接替代掉就好





## 参考

- [duchengyao/gpu-based-image-stitching: A simple version of "GPU based parallel optimization for real time panoramic video stitching".](https://github.com/duchengyao/gpu-based-image-stitching)
- [suncle1993/VideoStitching: solve real time video stitching problem： 4 camera example by opencv surf](https://github.com/suncle1993/VideoStitching)
- https://www.bilibili.com/video/BV1ri4y1s72t
- [AutoStitch笔记1 - 知乎](https://zhuanlan.zhihu.com/p/56633416)
- [文献汇总1](https://github.com/DoongLi/awesome-homography-estimation-and-image-alignment)
- [文献汇总2(来自综述论文)](https://github.com/MelodYanglc/Survey)
- [图像拼接论文、数据集汇总-CSDN博客](https://blog.csdn.net/qq_36584673/article/details/134711352)
- https://ppwwyyxx.com/blog/2016/How-to-Write-a-Panorama-Stitcher
- [深度学习版图像拼接_Seung-Yim Yau的博客-CSDN博客](https://blog.csdn.net/miracle0_0/category_7526176.html)
